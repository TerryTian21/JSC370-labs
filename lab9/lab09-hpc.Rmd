---
title: "Lab 9 - HPC"
output: 
html_document: default
link-citations: yes
---

# Learning goals

In this lab, you are expected to practice the following skills:

- Evaluate whether a problem can be parallelized or not.
- Practice with the parallel package.
- Use Rscript to submit jobs.

```{r include=FALSE}
# install any missing packages
# install.packages("microbenchmark")
# install.packages("parallel")

library(parallel)
library(microbenchmark)
library(ggplot2)
```

## Problem 1

Give yourself a few minutes to think about what you learned about parallelization. List three
examples of problems that you believe may be solved using parallel computing,
and check for packages on the HPC CRAN task view that may be related to it.

1. Matrix Operations and Arithmetic problems
2. Image and Signal Processing
3. Simulations i.e. (Monte Carlo, K-fold cross validation)

For example, the `parallel` package would allow us to run multiple simulations at the same time. `mclapply` is the paralleled version of `lapply` which enables us to apply a function over a list of vectors. Another example is using the `caret` package for cross validation or `boot` for bootstrapping.

`Rccparallel` (markov chains), `nimle` (baysian inference), `rstan` (Baysian Inference)

## Problem 2: Pre-parallelization

The following functions can be written to be more efficient without using
`parallel`:

1. This function generates a `n x k` dataset with all its entries having a Poisson distribution with mean `lambda`.

```{r p2-fun1, include = FALSE}

# For loop with rbind is inefficient (repeated reallocates memory as matrix grows)
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  
  for (i in 1:n)
    x <- rbind(x, rpois(k, lambda))
  
  return(x)
}

# Use matrix function - preallocate memory
fun1alt <- function(n = 100, k = 4, lambda = 4) {
  matrix(rpois(n*k, lambda = lambda), ncol = k)
}

# Benchmarking
microbenchmark::microbenchmark(
  fun1(100),
  fun1alt(100),
  unit = "us"
)
```

How much faster?

On average, `func1alt` is ~100 micro seconds faster than `func1`.

2.  Find the column max (hint: Checkout the function `max.col()`).

```{r p2-fun2, include = FALSE}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value. Loops through all columns and applys max on each column. Slower than direct matrix indexing
fun2 <- function(x) {
  apply(x, 2, max)
}

# Avoid function call inside loops
fun2alt <- function(x) {
  # YOUR CODE HERE
  x[cbind(max.col(t(x)), 1:ncol(x))]
}

# Benchmarking
res <- microbenchmark::microbenchmark(
  fun2(x),
  fun2alt(x),
  unit = "us"
)

```


```{r fig.width = 8, echo=FALSE}

library(gridExtra)

boxplot <- ggplot(res, aes(x=expr, y=time)) + 
  geom_boxplot() + 
  theme_bw() + 
  labs(
    title = "Boxplot comparing Function Runtimes"
  )

autoplot <- autoplot(res) + theme_bw()

grid.arrange(boxplot, autoplot, ncol = 2)

```


## Problem 3: Parallelize everything

We will now turn our attention to non-parametric 
[bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
Among its many uses, non-parametric bootstrapping allow us to obtain confidence
intervals for parameter estimates without relying on parametric assumptions.

The main assumption is that we can approximate many experiments by resampling
observations from our original dataset, which reflects the population. 

This function implements the non-parametric bootstrap:

```{r p3-boot-fun, include = FALSE}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
 
  # Making the cluster using `ncpus`
  # STEP 1: Create cluster for parallel computing using multiple cores
  # PSOCK = Parallel socket cluster
  # Create worker nodes
  clust <- makePSOCKcluster(ncpus)
  
  # STEP 2: 
  #Prevent memory leak - shut down cluster on exit
  # on.exit(stopcluster(clust))
  
  # Export variables to the cluster
  # Send variables to worker nodes - running in isolated env so they don't have access to global vars
  # idx - resampling indices for bootstrapping
  # dat - dataset
  # stat - statistical function used to compute estimates
  clusterExport(clust, varlist = c("idx", "dat", "stat"), envir = environment())
  
  
  # STEP 3: THIS FUNCTION NEEDS TO BE REPLACED WITH parLapply
  ans <- parLapply(clust, seq_len(R), function(i) {
    stat(dat[idx[,i], , drop=FALSE])
  })
  
  # Coercing the list into a matrix
  ans <- do.call(rbind, ans)
  
  # STEP 4: Free system resources
  stopCluster(clust)
  
  ans
  
}
```

1. Use the previous pseudocode, and make it work with `parallel`. Here is just an example for you to try:

```{r p3-test-boot, echo = FALSE}
# Bootstrap of a linear regression model
my_stat <- function(d) {coef(lm(y ~ x, data = d))} 

# DATA SIM
set.seed(1)
n <- 500 
R <- 1e4
x <- cbind(rnorm(n)) 
y <- x*5 + rnorm(n)

# Check if we get something similar as lm
# OLS Confidence Interval
ans0 <- confint(lm ( y ~ x))
cat("OLS CI \n")
print(ans0)

ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 4)
qs <- c(0.025, 0.975)
cat("Bootstrap CI \n")
print(t(apply(ans1, 2, quantile, probs=qs)))


```

2. Check whether your version actually goes faster than the non-parallel version:

```{r benchmark-problem3, echo=FALSE}
# your code here

parallel::detectCores()

# non-parallel 1 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))

# Parallel 4 core
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 8L))


```

Based on the output above, the function running on 8-cores runs faster than on a single core. The difference is around 0.5s.

## Problem 4: Compile this markdown document using Rscript

Once you have saved this Rmd file, try running the following command
in your terminal:

```bash
Rscript --vanilla -e 'rmarkdown::render("[full-path-to-your-Rmd-file.Rmd]")' &
```

Where `[full-path-to-your-Rmd-file.Rmd]` should be replace with the full path to
your Rmd file... :).


