---
title: "Lab 08 - Text Mining/NLP"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(eval = F, include  = T)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text
- Use dplyr and ggplot2 to analyze and visualize text data
- Try a theme model using `topicmodels`

# Lab description

For this lab we will be working with the medical record transcriptions from https://www.mtsamples.com/ available at https://github.com/JSC370/JSC370-2025/tree/main/data/medical_transcriptions.

# Deliverables

1. Questions 1-7 answered, knit to pdf or html output uploaded to Quercus.

2. Render the Rmarkdown document using `github_document` and add it to your github site. Add link to github site in your html.


### Setup packages

You should load in `tidyverse`, (or `data.table`), `tidytext`, `wordcloud2`, `tm`, and `topicmodels`.


## Read in the Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r eval=TRUE, message=FALSE, echo=FALSE}
# install.packages("tidytext")
# install.packages("wordcloud2")
# install.packages("tm")
# install.packages("topicmodels")
library(tidytext)
library(tidyverse)
library(wordcloud2)
library(tm)
library(topicmodels)
library(tokenizers)
library(stringr)
library(dplyr)
library(ggplot2)

mt_samples <- read_csv("https://raw.githubusercontent.com/JSC370/JSC370-2025/main/data/medical_transcriptions/mtsamples.csv")
mt_samples <- mt_samples |>
  select(description, medical_specialty, transcription)

head(mt_samples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different medical specialties are in the data. Are these categories related? overlapping? evenly distributed? Make a bar plot.

```{r eval=TRUE, message=FALSE, echo=FALSE}

mt_samples |>
  count(medical_specialty, sort = TRUE) |>
  ggplot(aes(fct_reorder(medical_specialty, n), n)) +
  geom_col() +
  coord_flip() +
  theme_bw()
```

There are 30 different medical specialties, with the data being heavily skewed in favor of the surgery category. Categories are independent and little overlap is observed.

---

## Question 2: Tokenize

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words with a bar plot
- Create a word cloud of the top 20 most frequent words

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r eval=TRUE, message=FALSE, echo=FALSE}
tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc))

top20 <- tokens |> head(20)

top20 |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_bar(stat="identity") +
  theme_bw() + 
  coord_flip()

top20 |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.5, color="random-light", background="grey")
```

From this result, the most common words are stopwords i.e. "the", "and", "was" ... etc. This makes snese, and we don't get a lot of meaningful insights. The word patient does come up quite often, indicating this is medical related data.


---

## Question 3: Stopwords

- Redo Question 2 but remove stopwords
- Check `stopwords()` library and `stop_words` in `tidytext`
- Use regex to remove numbers as well
- Try customizing your stopwords list to include 3-4 additional words that do not appear informative

### What do we see when you remove stopwords and then when you filter further? Does it give us a better idea of what the text is about?

```{r eval=TRUE, message=FALSE, echo=FALSE}
# head(stopwords("english"))
# length(stopwords("english"))
# head(stop_words)

tokens <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!str_detect(word, "['’]")) |> 
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  anti_join(stop_words, by = "word") |>
  group_by(word) |>
  summarise(word_frequency = n()) |>
  arrange(across(word_frequency, desc))

top20_nostop <- tokens |> head(20)

top20_nostop |>
  ggplot(aes(fct_reorder(word, word_frequency), word_frequency)) +
  geom_col() +
  theme_bw() + 
  coord_flip()

top20_nostop |>
  count(word, sort=TRUE) |>
  wordcloud2(size=0.5, color="random-light", background="grey")

```


Instead of prepositions, we see nouns, and verbs. For example, blood, skin, incision, provide better context of what topics the text is referring to.

---

## Question 4: ngrams

Repeat question 2, but this time tokenize into bi-grams. How does the result change if you look at tri-grams? Note we need to remove stopwords a little differently. You don't need to recreate the wordclouds.

```{r eval=TRUE, message=FALSE, echo=FALSE}

bigrams <- mt_samples$transcription |>
  tokenize_ngrams(n = 2) |>
  unlist() |>
  table() |>
  as.data.frame()

colnames(bigrams) <- c("bigram", "count")

```


```{r eval=TRUE, message=FALSE, echo=FALSE}


bigrams <- bigrams |> 
  separate(bigram, into = c("word1", "word2"), sep = " ", remove = FALSE) |>
  filter(!str_detect(bigram, "['’]")) |>
  filter(!str_detect(bigram, "[[:digit:]]+")) |>
  filter(!word1 %in% c("mm", "mg", "noted")) |>
  filter(!word2 %in% c("mm", "mg", "noted")) |>  
  anti_join(stop_words, by = c("word1" = "word")) |>
  anti_join(stop_words, by = c("word2" = "word")) |>
  arrange(desc(count))

top20_bigrams <- bigrams |> head(20)
top20_bigrams |>
  ggplot(aes(fct_reorder(bigram, count), count)) +
  geom_col() +
  coord_flip() +
  theme_bw()

```

Note: tutorial said to not compute tri-grams. In the bi-grams, the frequency counts are reduced and each bigram is more informative than singular words. Shows that medical terminology usually comes in phrases rather than single words.

---



## Question 5: Examining words


Using the results from the bigram, pick a word and count the words that appear before and after it, and create a plot of the top 20. 

```{r eval=TRUE, message=FALSE, echo=FALSE}
library(stringr)
# e.g. patient, blood, preoperative...

diagnosis <- bigrams |>
  filter(str_detect(bigram, "\\b diagnosis$|^diagnosis \\b")) |>
  mutate(target = ifelse(str_detect(word1, "diagnosis"), word2, word1)) |>
  group_by(target) |> 
  summarise(word_count = sum(count)) |>
  arrange(desc(word_count)) |>  
  head(20)

diagnosis %>% 
  ggplot(aes(fct_reorder(target, word_count), word_count)) +
  geom_col() +
  coord_flip() +
  theme_bw()
                  

```
Word of choice: diagnosis.

---


## Question 6: Words by Specialties

Which words are most used in each of the specialties? You can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the 5 most used words?


```{r eval=TRUE, message=FALSE, echo=FALSE}
specialty <- mt_samples |>
   unnest_tokens(word, transcription) |>
    filter(!str_detect(word, "['’]")) |> 
    filter(!str_detect(word, "[[:digit:]]+")) |>
    filter(!word %in% c("mm", "mg", "noted")) |>
    anti_join(stop_words, by = "word") |>
    group_by(medical_specialty, word) |>
    summarise(word_frequency = n()) 


specialty |>
    group_by(medical_specialty) |>
    slice_max(word_frequency, n = 5, with_ties = FALSE) |>
    arrange(medical_specialty, desc(word_frequency))

specialty |> arrange(desc(word_frequency))


```
The Allergy/Immunology specialty has top 5 words of allergies, sprays, prescription compared to Dentistry which has words like tooth, and removed. This makes sense. 
The top 5 most used words are patient, left, procedure, anesthesia, incision.

## Question 7: Topic Models

See if there are any themes in the data by using a topic model (LDA). 

- you first need to create a document term matrix
- then you can try the LDA function in `topicmodels`. Try different k values.
- create a facet plot of the results from the LDA (see code from lecture)


```{r eval=TRUE, message=FALSE, echo=FALSE}

# install.packages("reshape2")
library(reshape2)

transcripts_dtm <- mt_samples |>
  select(transcription) |>
  unnest_tokens(word, transcription) |>
  filter(!str_detect(word, "['’]")) |> 
  filter(!str_detect(word, "[[:digit:]]+")) |>
  filter(!word %in% c("mm", "mg", "noted")) |>
  anti_join(stop_words, by = "word") |>
  DocumentTermMatrix() |>
  as.matrix()


lda_model <- LDA(transcripts_dtm, k=5, control=list(seed=42))

top_terms <- 
  tidy(lda_model, matrix="beta") |>
  group_by(topic) |>
  slice_max(beta, n=10) |>
  ungroup() |>
  arrange(topic, -beta)

top_terms |>
  mutate(term=reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill=factor(topic))) + 
  geom_col(snow.legend=FALSE) +
  facet_wrap(~topic, scales="free") +
  scale_y_reordered() +
  theme_bw()
```

Sorting the words into categories we can observe "themes". While many of the topics share similar words we can differentiate via others like abdomen in topic 4 and artery in topic 5. This gives evidence for procedures on different parts of the body.



